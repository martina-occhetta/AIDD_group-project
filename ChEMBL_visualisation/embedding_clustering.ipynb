{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "657d78c6-fe0a-45c7-a390-947af5642a39",
   "metadata": {},
   "source": [
    "## Clustering using embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2c53c3-c31b-41fa-bdde-8f9fb83ae34c",
   "metadata": {},
   "source": [
    "### Load embeddings data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1d848f-bc9d-4413-9996-572b9d37533b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "processed_data_embeddings = pd.read_csv(\"assays_with_embeddings.csv\")\n",
    "processed_data_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea9fff4-4376-4ff9-b346-3e1874847b8c",
   "metadata": {},
   "source": [
    "### Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66dc5d4-a092-4499-800f-25151567e4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# for some reason embeddings list is a string, needs to be converted\n",
    "processed_data_embeddings['embeddings'].replace('\\[','', regex=True, inplace=True)\n",
    "processed_data_embeddings['embeddings'].replace('\\]','', regex=True, inplace=True)\n",
    "processed_data_embeddings['embeddings'].replace('\\n','', regex=True, inplace=True)\n",
    "processed_data_embeddings['embeddings'] = processed_data_embeddings['embeddings'].str.strip()\n",
    "processed_data_embeddings['embeddings'] = processed_data_embeddings['embeddings'].str.split(' ')\n",
    "\n",
    "# loops to remove empty elements from embeddings and converts embeddings to np array\n",
    "new_embeddings = []\n",
    "for embedding_list in processed_data_embeddings['embeddings'].values:\n",
    "    # using lambda function\n",
    "    embedding_list_ = list(itertools.filterfalse(lambda x: x == '', embedding_list))\n",
    "    # convert list to np array\n",
    "    embedding_array = np.array(embedding_list_)\n",
    "    # cast elements to float\n",
    "    embedding_array = embedding_array.astype(float) \n",
    "    # append to list of arrays\n",
    "    new_embeddings.append(embedding_array)\n",
    "    \n",
    "# swap in reformatted embeddings\n",
    "processed_data_embeddings['embeddings'] = new_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c699f87-8709-4d3e-a592-a1f096c01331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of embedding\n",
    "print(processed_data_embeddings['embeddings'][3].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0688fb72-7bab-4a09-b761-630c45fdceeb",
   "metadata": {},
   "source": [
    "### Find embeddings distance and create distance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb368881-8369-49b6-b482-1107961a251d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319a72c1-e7d2-470d-909d-4b5ac33a0ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example cosine_similarity calculation\n",
    "# Word embeddings for two words\n",
    "word1_embedding = processed_data_embeddings['embeddings'][0]\n",
    "word2_embedding = processed_data_embeddings['embeddings'][1000]\n",
    "# Reshape the arrays to match the expected input shape of cosine_similarity\n",
    "word1_embedding = word1_embedding.reshape(1, -1)\n",
    "word2_embedding = word2_embedding.reshape(1, -1)\n",
    "# Calculate cosine similarity\n",
    "similarity = cosine_similarity(word1_embedding, word2_embedding)[0][0]\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d47854-1d3c-4763-91c1-bf7a45419fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for speed, drop duplicate assays as rows were duplicated for each compound\n",
    "processed_data_embeddings_ = processed_data_embeddings # store copy of orginal dataframe\n",
    "processed_data_embeddings = processed_data_embeddings.drop_duplicates(subset=['assay_chembl_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa650384-7775-436a-8c9a-4060421eec7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_cols = processed_data_embeddings.assay_chembl_id.to_list()\n",
    "similarity_index = processed_data_embeddings.assay_chembl_id.to_list()\n",
    "embeddings_list = processed_data_embeddings.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853e11ef-96b4-42ca-b399-49ec5c44a44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of IDs to calculate cosine similarity for\n",
    "n = len(embeddings_list) # for faster example, change 'n' to something â‰ˆ 500-1000, otherwise -> len(embeddings_list)\n",
    "# create list to store all lists of embedding similarities\n",
    "similarity_frame = []\n",
    "\n",
    "# iterate over list of embeddings, assign embedding as embedding_a\n",
    "# NOTE!!!: This takes some time, have to calculate consine similarities for n_embeddings^2 (I think 5000 unique description embeddins takes roughly 1 hour using 6 cores of 2GB memory on CPU)\n",
    "for count, embedding_a in enumerate(embeddings_list[:n],1):\n",
    "    # create list to store all embedding cosine similarities for embedding_a\n",
    "    similarity_list = []\n",
    "    # iterate over list of embeddings, assign embedding as embedding_b, for compariosn\n",
    "    for embedding_b in embeddings_list[:n]:\n",
    "        # Reshape the arrays to match the expected input shape of cosine_similarity\n",
    "        embedding_a = embedding_a.reshape(1, -1)\n",
    "        embedding_b = embedding_b.reshape(1, -1)\n",
    "        # Calculate cosine similarity\n",
    "        similarity = cosine_similarity(embedding_a, embedding_b)[0][0]\n",
    "        # append value to list\n",
    "        similarity_list.append(similarity)\n",
    "    # append similarity list for embedding_a to list of lists\n",
    "    similarity_frame.append(similarity_list)\n",
    "    # print count if multiple of 100\n",
    "    if count % 100 == 0:\n",
    "        print(f\"Iteration {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a8c261-4efc-40e6-a97e-5349369834c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create cosine similarity matrix\n",
    "cosine_similarity_matrix = pd.DataFrame(similarity_frame,columns=similarity_cols[:n],index=similarity_index[:n])\n",
    "cosine_similarity_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f698cb4-b393-4de1-821b-1547e2057d60",
   "metadata": {},
   "source": [
    "### Clustering using similarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59ff91e-4ed2-42c2-96b1-dc21217e23e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Agglomerative clustering (can swap out for other clustering methods/parameters)\n",
    "clustering = AgglomerativeClustering(n_clusters=100).fit(cosine_similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ec4c2a-703f-4c42-992c-d92381bcf125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset for example chembl IDs, will also work if using all embeddings\n",
    "data_subset = processed_data_embeddings[processed_data_embeddings['assay_chembl_id'].isin(cosine_similarity_matrix.columns)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c607134b-5932-45cf-a7ca-bdcc48550b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create column for cluster label\n",
    "data_subset['embedding_cluster'] = clustering.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27fdb5f-b939-45cb-8879-1536a7239e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram plot to check distribution (should be relatively even, if not, clustering is likely bad)\n",
    "from matplotlib import pyplot as plt \n",
    "import numpy as np  \n",
    "   \n",
    "plt.hist(clustering.labels_) \n",
    "plt.title(\"histogram\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d45640f-6fb0-4889-9687-fab7ef1d4b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if clustering is giving useful groupings\n",
    "for row in data_subset[data_subset.embedding_cluster==0]['description']:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e2441f-106a-43dd-bf15-0499b696e980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if clustering is giving useful groupings\n",
    "for row in data_subset[data_subset.embedding_cluster==10]['description']:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d586b387-d906-40d0-9d59-a3054c68f829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if clustering is giving useful groupings\n",
    "for row in data_subset[data_subset.embedding_cluster==20]['description']:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a432db-e694-4f31-97c3-ba9a222167e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dictionary mapping assay ID to respective cluster\n",
    "# store assay IDs as keys and cluster label as values\n",
    "keys = data_subset.assay_chembl_id\n",
    "vals = data_subset.embedding_cluster\n",
    "# using dict() and zip() to convert lists to dictionary\n",
    "cluster_dict = dict(zip(keys, vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e96b330-400b-4ec9-9bae-31b620b9e810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract list of assays IDs\n",
    "all_assay_IDs = processed_data_embeddings_.assay_chembl_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bd122a-65cd-408f-aa55-77e47dae69d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cluster_labels = []\n",
    "for ID in all_assay_IDs:\n",
    "    if ID in cluster_dict.keys():\n",
    "        cluster_ID = cluster_dict[ID]\n",
    "        all_cluster_labels.append(cluster_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac78382-6bbc-4bb6-8ea3-8236e7f91aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create column for cluster labels in expanded dataframe\n",
    "processed_data_embeddings_['embedding_cluster'] = all_cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4264560-a397-4a8c-8353-0b967ec74f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_embeddings_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
